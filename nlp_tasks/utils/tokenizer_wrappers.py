# -*- coding: utf-8 -*-


from keras.preprocessing import text as keras_text_processor

from nlp_tasks.utils import word_processor
from nlp_tasks.utils import tokenizers


class TokenizerWithCustomWordSegmenter(keras_text_processor.Tokenizer):
    """

    """

    def __init__(self, word_segmenter, num_words=None,
                 filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                 lower=True,
                 split=' ',
                 char_level=False,
                 oov_token=None,
                 **kwargs):
        super().__init__(num_words=num_words,
                 filters=filters,
                 lower=lower,
                 split=split,
                 char_level=char_level,
                 oov_token=oov_token,
                 **kwargs)
        self.word_segmenter = word_segmenter

    def text_to_word_sequence(self, text):
        """

        :param text:
        :return:
        """
        return self.word_segmenter(text)

    def fit_on_texts(self, texts):
        """Updates internal vocabulary based on a list of texts.

        In the case where texts contains lists,
        we assume each entry of the lists to be a token.

        Required before using `texts_to_sequences` or `texts_to_matrix`.

        # Arguments
            texts: can be a list of strings,
                a generator of strings (for memory-efficiency),
                or a list of list of strings.
        """
        for text in texts:
            self.document_count += 1
            if self.char_level or isinstance(text, list):
                if self.lower:
                    if isinstance(text, list):
                        text = [text_elem.lower() for text_elem in text]
                    else:
                        text = text.lower()
                seq = text
            else:
                seq = self.text_to_word_sequence(text)
            for w in seq:
                if w in self.word_counts:
                    self.word_counts[w] += 1
                else:
                    self.word_counts[w] = 1
            for w in set(seq):
                # In how many documents each word occurs
                self.word_docs[w] += 1

        wcounts = list(self.word_counts.items())
        wcounts.sort(key=lambda x: x[1], reverse=True)
        # forcing the oov_token to index 1 if it exists
        if self.oov_token is None:
            sorted_voc = []
        else:
            sorted_voc = [self.oov_token]
        sorted_voc.extend(wc[0] for wc in wcounts)

        # note that index 0 is reserved, never assigned to an existing word
        self.word_index = dict(
            list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))

        self.index_word = dict((c, w) for w, c in self.word_index.items())

        for w, c in list(self.word_docs.items()):
            self.index_docs[self.word_index[w]] = c

    def text_to_sequence(self, text):
        sequences = self.texts_to_sequences([text])
        return sequences[0]

    def texts_to_sequences_generator(self, texts):
        """Transforms each text in `texts` to a sequence of integers.

        Each item in texts can also be a list,
        in which case we assume each item of that list to be a token.

        Only top "num_words" most frequent words will be taken into account.
        Only words known by the tokenizer will be taken into account.

        # Arguments
            texts: A list of texts (strings).

        # Yields
            Yields individual sequences.
        """
        num_words = self.num_words
        oov_token_index = self.word_index.get(self.oov_token)
        for text in texts:
            if self.char_level or isinstance(text, list):
                if self.lower:
                    if isinstance(text, list):
                        text = [text_elem.lower() for text_elem in text]
                    else:
                        text = text.lower()
                seq = text
            else:
                seq = self.text_to_word_sequence(text)
            vect = []
            for w in seq:
                i = self.word_index.get(w)
                if i is not None:
                    if num_words and i >= num_words:
                        if oov_token_index is not None:
                            vect.append(oov_token_index)
                    else:
                        vect.append(i)
                elif self.oov_token is not None:
                    vect.append(oov_token_index)
            yield vect


if __name__ == '__main__':
    word_processor1 = word_processor.LowerProcessor()
    word_segmenter = tokenizers.NltkTokenizer(word_processor=word_processor1)
    keras_tokenizer = TokenizerWithCustomWordSegmenter(tokenizers.JiebaTokenizer())
    keras_tokenizer.fit_on_texts(['我在深圳上班',
                                  'Boot time is super fast, around anywhere from 35 seconds to 1 minute.'])
    keras_tokenizer.texts_to_sequences()
